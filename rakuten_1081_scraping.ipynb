{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2863076",
   "metadata": {},
   "source": [
    "# Rakuten Travel ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ2024å¹´9æœˆï¼‰\n",
    "\n",
    "ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€æ¥½å¤©ãƒˆãƒ©ãƒ™ãƒ«ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹å…¨å›½ã®ãƒ›ãƒ†ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã†ã¡ã€**2024å¹´9æœˆã«æŠ•ç¨¿ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼**ã‚’éƒ½é“åºœçœŒã”ã¨ã«**æœ€å¤§23ä»¶ãšã¤**åé›†ã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã§ã™ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å®Ÿè¡Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "å„çœŒã®åé›†ã™ã‚‹ä»¶æ•°ã‚’å¢—ã‚„ã™ã¨ãã¯é©å®œè©²å½“ç®‡æ‰€ã‚’ç·¨é›†ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",

    "---\n",
    "\n",
    "## ç‰¹å¾´\n",
    "\n",
    "- **å¯¾è±¡æœŸé–“**ï¼š2024å¹´9æœˆã«å®¿æ³Šã—ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "- **å¯¾è±¡åœ°åŸŸ**ï¼šå…¨å›½47éƒ½é“åºœçœŒï¼ˆå„çœŒæœ€å¤§23ä»¶ï¼‰\n",
    "- **å¯¾è±¡ãƒšãƒ¼ã‚¸**ï¼š`https://travel.rakuten.co.jp/yado/<slug>/` ã‚’ä¸­å¿ƒã«è¤‡æ•°ã®ãƒªã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ãƒ›ãƒ†ãƒ«ä¸€è¦§ã‚’å–å¾—\n",
    "- **ä¸¦åˆ—å‡¦ç†**ï¼š`ThreadPoolExecutor(max_workers=8)` ã«ã‚ˆã‚Šã€ãƒ›ãƒ†ãƒ«è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã‚’é«˜é€ŸåŒ–\n",
    "- **é€²æ—è¡¨ç¤º**ï¼šæ¨™æº–å‡ºåŠ›ã«ã¦é€²æ—çŠ¶æ³ã‚’è¡¨ç¤ºã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹çœŒã¯âš ãƒãƒ¼ã‚¯ä»˜ãã§é€šçŸ¥\n",
    "- **ä¿å­˜å½¢å¼**ï¼š`japan_202409_reviews.json` ã«çµæœã‚’ä¿å­˜ï¼ˆUTF-8ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "RakutenÂ TravelÂ ãƒ¬ãƒ“ãƒ¥ãƒ¼åé›† 2024â€‘09ï¼ˆå…¨å›½ãƒ»çœŒåˆ¥ 23 ä»¶ä¸Šé™ãƒ»é€²æ—è¡¨ç¤ºï¼‰\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â— å„éƒ½é“åºœçœŒ 23 ä»¶ï¼šæœ¬æ–‡ã‚ã‚Š / å®¿æ³Šå¹´æœˆ 2024â€‘09\n",
    "â— /yado/<slug>/ ã‚’ãƒ¡ã‚¤ãƒ³ä¸€è¦§ã¨ã—ã¦ä½¿ç”¨\n",
    "â— ThreadPoolExecutor(max_workers=8) ã§è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ä¸¦åˆ—å–å¾—\n",
    "â— çœŒã”ã¨ã«ä¸è¶³ãŒã‚ã‚Œã° âš  ã‚’è¡¨ç¤ºã—ã€çµ‚äº†æ™‚ã«ã‚µãƒãƒª\n",
    "\"\"\"\n",
    "\n",
    "import re, json, time, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_PER_PREF = 23\n",
    "TARGET_MONTH = \"2024å¹´09æœˆ\"\n",
    "MAX_WORKERS  = 8\n",
    "SLEEP_RANGE  = (0.2, 0.5)\n",
    "OUT_FILE     = \"japan_202409_reviews.json\"\n",
    "UA           = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "PREF_SLUGS = [\n",
    "    \"hokkaido\",\"aomori\",\"iwate\",\"miyagi\",\"akita\",\"yamagata\",\"fukushima\",\n",
    "    \"ibaraki\",\"tochigi\",\"gunma\",\"saitama\",\"chiba\",\"tokyo\",\"kanagawa\",\n",
    "    \"niigata\",\"toyama\",\"ishikawa\",\"fukui\",\"yamanashi\",\"nagano\",\n",
    "    \"gifu\",\"shizuoka\",\"aichi\",\"mie\",\n",
    "    \"shiga\",\"kyoto\",\"osaka\",\"hyogo\",\"nara\",\"wakayama\",\n",
    "    \"tottori\",\"shimane\",\"okayama\",\"hiroshima\",\"yamaguchi\",\n",
    "    \"tokushima\",\"kagawa\",\"ehime\",\"kochi\",\n",
    "    \"fukuoka\",\"saga\",\"nagasaki\",\"kumamoto\",\"oita\",\"miyazaki\",\n",
    "    \"kagoshima\",\"okinawa\"\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTP with retry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_session = requests.Session()\n",
    "def http_get(url: str, **kw):\n",
    "    for back in (1, 2, 4):\n",
    "        try:\n",
    "            r = _session.get(url, headers=UA, timeout=10, **kw)\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(back)\n",
    "    return None\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hotel list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PAT_ID = re.compile(r\"/(?:HOTEL|hotelinfo)/(\\d+)/\", re.I)\n",
    "\n",
    "LIST_PATTERNS = [\n",
    "    # â‘  ç¾è¡Œ /yado/<slug>/ ï¼ˆãƒšãƒ¼ã‚¸ 2 ä»¥é™ã¯ ?page=2ï¼‰\n",
    "    \"https://travel.rakuten.co.jp/yado/{slug}/?page={page}\",\n",
    "    # â‘¡ ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥æ—§ URL\n",
    "    \"https://travel.rakuten.co.jp/group/tiku/02japan{slug}.html?p={page}\",\n",
    "    # â‘¢ äº‹å‹™æ‰€åˆ¥æ—§ URL\n",
    "    \"https://travel.rakuten.co.jp/office/pref/{slug}.html?p={page}\",\n",
    "    # â‘£ è¦³å…‰ãƒšãƒ¼ã‚¸\n",
    "    \"https://travel.rakuten.co.jp/group/kanko/02japan{slug}.html?p={page}\",\n",
    "    # â‘¤ æ¸©æ³‰ãƒšãƒ¼ã‚¸\n",
    "    \"https://travel.rakuten.co.jp/group/onsen/02japan{slug}.html?p={page}\",\n",
    "]\n",
    "\n",
    "def fetch_hotel_ids(slug: str, max_pages: int = 50):\n",
    "    ids = set()\n",
    "    for base in LIST_PATTERNS:\n",
    "        for p in range(1, max_pages + 1):\n",
    "            url = base.format(slug=slug, page=p)\n",
    "            res = http_get(url)\n",
    "            if not res:\n",
    "                break\n",
    "            ids.update(PAT_ID.findall(res.text))\n",
    "            # ã€Œæ¬¡ã¸ã€ãƒªãƒ³ã‚¯ãŒç„¡ã‘ã‚Œã°æ‰“ã¡åˆ‡ã‚Š\n",
    "            if \"æ¬¡ã¸\" not in res.text and \"?page=\" not in res.text:\n",
    "                break\n",
    "            time.sleep(random.uniform(*SLEEP_RANGE))\n",
    "        if ids:          # ã©ã‚Œã‹ 1 ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–ã‚Œã‚Œã°ååˆ†\n",
    "            break\n",
    "    return list(ids)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è©³ç´°ãƒšãƒ¼ã‚¸ â†’ ãƒ¬ãƒ“ãƒ¥ãƒ¼ 1 ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LINK_SEL = (\n",
    "    \"p.reviewTitle a, h2.commentTitle a, \"\n",
    "    \"div.rvTtl a, div.rvTltl a\"\n",
    ")\n",
    "\n",
    "def fetch_review(hid: str, slug: str):\n",
    "    lst = http_get(\n",
    "        f\"https://review.travel.rakuten.co.jp/hotel/voice/{hid}/\",\n",
    "        params={\"f_time\":\"202409\",\"f_sort\":\"0\",\"f_next\":\"0\"}\n",
    "    )\n",
    "    if not lst:\n",
    "        return None, \"ä¸€è¦§å–å¾—å¤±æ•—\"\n",
    "\n",
    "    soup = BeautifulSoup(lst.text, \"html.parser\")\n",
    "    a = soup.select_one(LINK_SEL)\n",
    "    if not a:\n",
    "        return None, \"è©³ç´°ãƒªãƒ³ã‚¯ç„¡ã—\"\n",
    "\n",
    "    det_url = a[\"href\"]\n",
    "    if det_url.startswith(\"/\"):\n",
    "        det_url = \"https://review.travel.rakuten.co.jp\" + det_url\n",
    "    det = http_get(det_url)\n",
    "    if not det:\n",
    "        return None, \"è©³ç´°å–å¾—å¤±æ•—\"\n",
    "\n",
    "    sd = BeautifulSoup(det.text, \"html.parser\")\n",
    "    body = sd.select_one(\"p.commentSentence\")\n",
    "    if not body or not body.get_text(strip=True):\n",
    "        return None, \"æœ¬æ–‡ç©º\"\n",
    "\n",
    "    dds = sd.select(\"dl.commentPurpose dd\")\n",
    "    stay = dds[3].get_text(strip=True) if len(dds) >= 4 else \"\"\n",
    "    if TARGET_MONTH not in stay:\n",
    "        return None, \"9æœˆä»¥å¤–\"\n",
    "\n",
    "    return {\n",
    "        \"hotel_id\": hid,\n",
    "        \"prefecture\": slug,\n",
    "        \"stay_date\": stay,\n",
    "        \"review_text\": body.get_text(strip=True)\n",
    "    }, None\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    start = time.time()\n",
    "    results, used_hotels = [], set()\n",
    "    err_counter = defaultdict(int)\n",
    "\n",
    "    with ThreadPoolExecutor(MAX_WORKERS) as pool:\n",
    "        for slug in PREF_SLUGS:\n",
    "            print(f\"ğŸŒ {slug}\")\n",
    "            hotel_ids = fetch_hotel_ids(slug)\n",
    "            if not hotel_ids:\n",
    "                print(\"  âš  hotel_ids 0 ä»¶\")\n",
    "                err_counter[(slug, \"hotel_ids 0 ä»¶\")] += 1\n",
    "                continue\n",
    "            random.shuffle(hotel_ids)\n",
    "\n",
    "            futs = []\n",
    "            for hid in hotel_ids:\n",
    "                if len([r for r in results if r['prefecture'] == slug]) >= MAX_PER_PREF:\n",
    "                    break\n",
    "                if hid in used_hotels:\n",
    "                    continue\n",
    "                futs.append(pool.submit(fetch_review, hid, slug))\n",
    "                used_hotels.add(hid)\n",
    "\n",
    "            success = 0\n",
    "            for fut in as_completed(futs):\n",
    "                rev, err = fut.result()\n",
    "                if rev:\n",
    "                    results.append(rev)\n",
    "                    success += 1\n",
    "                    print(f\"  âœ” {rev['hotel_id']} {rev['stay_date']}\")\n",
    "                else:\n",
    "                    err_counter[(slug, err)] += 1\n",
    "\n",
    "                if success >= MAX_PER_PREF:\n",
    "                    break\n",
    "\n",
    "            if success < MAX_PER_PREF:\n",
    "                print(f\"  âš  {MAX_PER_PREF - success} ä»¶ä¸è¶³ ({success}/{MAX_PER_PREF})\")\n",
    "\n",
    "    # ä¿å­˜\n",
    "    Path(OUT_FILE).write_text(\n",
    "        json.dumps(results, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # ã‚µãƒãƒª\n",
    "    print(\"\\nâ”â” ã‚µãƒãƒª â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "    goal = MAX_PER_PREF * len(PREF_SLUGS)\n",
    "    print(f\"æœŸå¾…: {goal} ä»¶ | å®Ÿéš›: {len(results)} ä»¶\")\n",
    "    if err_counter:\n",
    "        print(\"ä¸è¶³ç†ç”± TOP5\")\n",
    "        for (slug, reason), cnt in sorted(err_counter.items(), key=lambda x: -x[1])[:5]:\n",
    "            print(f\"  {reason} [{slug}] â€¦ {cnt} ä»¶\")\n",
    "    print(f\"ä¿å­˜å…ˆ: {OUT_FILE} (çµŒé {(time.time() - start)/60:.1f} åˆ†)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
